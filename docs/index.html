<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="main.css" />
    <title>Transformer Attention</title>
  </head>
  <body>
    <div class="container">
      <div class="heading">
        <h1>Hello!</h1>
        <p><span>About Transformer & Attention</span></p>
      </div>
      <hr />
      <p>
        Is the point of this page just a fancy way of writing the
        <a href="https://github.com/12dash/TransformerAttention">Readme</a>?
      </p>
      <h4>Attention</h4>
      <p>
        The attention mechanism was introduced in this paper
        <a href="https://arxiv.org/pdf/1706.03762.pdf"
          >Attention is all you need!</a
        >
        that provided competition to the then state of the art Seq2Seq modeling
        components like Recurrent Neural Network such as LSTM, GRU. As the name
        suggests, <b>Attention is all you need</b> to capture sequential
        information.
      </p>
      <p>
        Let's get into some equations now. We have three variables in
        <b>Scaled Dot Product Attention</b>: \(Q \in \mathbb{R}^{d}, < K \in
        \mathbb{R}^{T \times d}, V \in \mathbb{R}^{T \times d_v}> \)
      </p>
      <p class="equations">
        $$ \text{Un-normalized Attention} = \text{Q} \cdot \text{K}^T$$
      </p>
      <p>
        where Q is the query vector, K is sequence of Key vector and d is the
        dimension of the vectors. T is the total number of time steps or
        sequence length we are considering.
      </p>
      <p class="intuition">
        The intuition behind this is to find cosine similarity between the two
        vector \( \text{Q }\& \text{ K}_t \) at timestep t. The cosine
        similarlity acts as a weightage to the corresponding value vector i.e.
        \( V_t \)
      </p>
      <p>
        Since these are weightage we would like the scores to be somewhat
        smaller in value ideally close to the range \([0, 3] \) The upper bound
        3 is arbitary. I just came up with that : ). The point is we want scores
        that are somewhat close to each other in magnitude and the lowest is 0
        which means do not given any attention to that timestamp in the
        sequence. Hence we normalize the above quantity by taking a softmax. The
        factor of \(\sqrt{d}\) is also multiplied to reduce the magnitude so
        that it does not push the values of the softmax in regions that have
        very small gradients.
      </p>
      <p class="equations">
        $$\text{Normalized Attention} = \text{softmax}(\frac{\text{Q} \cdot
        \text{K}^T}{\sqrt{d}})$$ $$\text{Attention Value} =
        \text{softmax}(\frac{\text{Q} \cdot \text{K}^T}{\sqrt{d}})V$$
      </p>
    </div>
  </body>
  <script
    src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
    crossorigin="anonymous"
  ></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
  ></script>
</html>
