# Positional Encodings
The positional embedding generated using the $\sin$ and $\cos$ functions.  
Link to the code [Positional Encoding.ipynb](PositionalEncoding.ipynb)
<img src='imgs/positional embedding.png'>

# Multi-head Attention 
Scaled dot-product attention is implemented in a multi-head attention mechanism.  
Link to the code [Multi Head Attention](Attention.ipynb)  
<img src='imgs/attention weights.png'>

# Transformer Architecture
Builduing the encoder and decoder comprising of the multi-head attentions and the fc layers.   
Link to the code [Transformer]("TransformerStack.ipynb")