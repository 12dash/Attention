# Positional Encodings
The positional embedding generated using the $\sin$ and $\cos$ functions.  
Link to the code [Positional Encoding.ipynb](PositionalEncoding.ipynb)  
<img src='notebooks/imgs/positionalembedding.png' style='max-wdith:100%;' alt='click on this'>

# Multi-head Attention 
Scaled dot-product attention is implemented in a multi-head attention mechanism.  
Link to the code [Multi Head Attention](Attention.ipynb)  
<img src='imgs/attention weights.png'>

# Transformer Model
There is a sample Transformer model that I created. Link to the code [Transformer](Transformer.ipynb)