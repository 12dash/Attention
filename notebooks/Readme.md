# Positional Encodings
The positional embedding generated using the $\sin$ and $\cos$ functions.  
Link to the code [Positional Encoding.ipynb](PositionalEncoding.ipynb)
<img src='imgs/positional embedding.png'>

# Multi-head Attention 
Scaled dot-product attention is implemented in a multi-head attention mechanism.  
Link to the code [Multi Head Attention](Attention.ipynb)  
<img src='imgs/attention weights.png'>