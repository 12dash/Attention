# Transformers & Attention
The repository is meant to understand how attention model works. I plan to implement the basic structure of attention and transformer model from the paper [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## To-Do
- [ ] Implementation of the transformer from the paper.
    - [X] Positional Encoding
- [ ] Basic Examples to train :
    - [ ] Time series model to train as auto-regressive
    - [ ] NLP tasks